{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0ef795-adcf-4c4a-9e78-b404e0d34b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Logging setup (self-contained)\n",
    "# -------------------------------\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import uuid\n",
    "\n",
    "LOG_PATH = \"/FileStore/project/logs/pipeline_logs\"\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField(\"run_id\", StringType()),\n",
    "    StructField(\"pipeline_layer\", StringType()),\n",
    "    StructField(\"notebook_name\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),   # START / END / ERROR / REJECTED\n",
    "    StructField(\"record_count\", LongType()),\n",
    "    StructField(\"status\", StringType()),       # RUNNING / SUCCESS / FAILED\n",
    "    StructField(\"error_message\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Create logging table if it does not exist\n",
    "if not spark._jsparkSession.catalog().tableExists(\"delta.`/FileStore/project/logs/pipeline_logs`\"):\n",
    "    spark.createDataFrame([], log_schema) \\\n",
    "        .write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(LOG_PATH)\n",
    "\n",
    "def log_event(layer, notebook, event_type, record_count, status, error_msg=None):\n",
    "\n",
    "    row = [(\n",
    "        str(uuid.uuid4()),\n",
    "        layer,\n",
    "        notebook,\n",
    "        event_type,\n",
    "        int(record_count),\n",
    "        status,\n",
    "        error_msg,          # can be None safely now\n",
    "        None                # placeholder for timestamp\n",
    "    )]\n",
    "\n",
    "    df = spark.createDataFrame(row, schema=log_schema) \\\n",
    "              .withColumn(\"event_timestamp\", current_timestamp())\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").save(LOG_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846435b2-b7d7-4bf3-aa6c-dba9553c9e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 5.1: Start MERGE logic\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "notebook_name = \"silver_merge\"\n",
    "log_event(\"silver\", notebook_name, \"START\", 0, \"RUNNING\")\n",
    "\n",
    "silver_delta = DeltaTable.forPath(\n",
    "    spark, \"/FileStore/project/silver/sales\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf55cfd-ab83-404c-82d4-4c3bd2a86bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4725766364173296>, line 5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Task 5.2: Perform MERGE (UPSERT)\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m      4\u001B[0m     silver_delta\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n",
       "\u001B[0;32m----> 5\u001B[0m         source\u001B[38;5;241m=\u001B[39mvalid_sales\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      6\u001B[0m         condition\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt.transaction_id = s.transaction_id\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m     )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n",
       "\u001B[1;32m      8\u001B[0m      \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n",
       "\u001B[1;32m      9\u001B[0m      \u001B[38;5;241m.\u001B[39mexecute()\n",
       "\u001B[1;32m     11\u001B[0m     log_event(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msilver\u001B[39m\u001B[38;5;124m\"\u001B[39m, notebook_name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEND\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     12\u001B[0m               valid_sales\u001B[38;5;241m.\u001B[39mcount(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSUCCESS\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'valid_sales' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'valid_sales' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'valid_sales' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-4725766364173296>, line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Task 5.2: Perform MERGE (UPSERT)\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      4\u001B[0m     silver_delta\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[0;32m----> 5\u001B[0m         source\u001B[38;5;241m=\u001B[39mvalid_sales\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      6\u001B[0m         condition\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt.transaction_id = s.transaction_id\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m     )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n\u001B[1;32m      8\u001B[0m      \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n\u001B[1;32m      9\u001B[0m      \u001B[38;5;241m.\u001B[39mexecute()\n\u001B[1;32m     11\u001B[0m     log_event(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msilver\u001B[39m\u001B[38;5;124m\"\u001B[39m, notebook_name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEND\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     12\u001B[0m               valid_sales\u001B[38;5;241m.\u001B[39mcount(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSUCCESS\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "\u001B[0;31mNameError\u001B[0m: name 'valid_sales' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 5.2: Perform MERGE (UPSERT)\n",
    "\n",
    "try:\n",
    "    silver_delta.alias(\"t\").merge(\n",
    "        source=valid_sales.alias(\"s\"),\n",
    "        condition=\"t.transaction_id = s.transaction_id\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "    log_event(\"silver\", notebook_name, \"END\",\n",
    "              valid_sales.count(), \"SUCCESS\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_event(\"silver\", notebook_name, \"ERROR\", 0, \"FAILED\", str(e))\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "incremental merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}