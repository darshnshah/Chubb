{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7541ad13-1c03-418c-99bb-2ce96526e7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Logging setup (self-contained)\n",
    "# -------------------------------\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import uuid\n",
    "\n",
    "LOG_PATH = \"/FileStore/project/logs/pipeline_logs\"\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField(\"run_id\", StringType()),\n",
    "    StructField(\"pipeline_layer\", StringType()),\n",
    "    StructField(\"notebook_name\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),   # START / END / ERROR / REJECTED\n",
    "    StructField(\"record_count\", LongType()),\n",
    "    StructField(\"status\", StringType()),       # RUNNING / SUCCESS / FAILED\n",
    "    StructField(\"error_message\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Create logging table if it does not exist\n",
    "if not spark._jsparkSession.catalog().tableExists(\"delta.`/FileStore/project/logs/pipeline_logs`\"):\n",
    "    spark.createDataFrame([], log_schema) \\\n",
    "        .write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(LOG_PATH)\n",
    "\n",
    "def log_event(layer, notebook, event_type, record_count, status, error_msg=None):\n",
    "\n",
    "    row = [(\n",
    "        str(uuid.uuid4()),\n",
    "        layer,\n",
    "        notebook,\n",
    "        event_type,\n",
    "        int(record_count),\n",
    "        status,\n",
    "        error_msg,          # can be None safely now\n",
    "        None                # placeholder for timestamp\n",
    "    )]\n",
    "\n",
    "    df = spark.createDataFrame(row, schema=log_schema) \\\n",
    "              .withColumn(\"event_timestamp\", current_timestamp())\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").save(LOG_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ab2cd3-f518-433c-9388-c06d1e22f51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.1: Initialize Bronze ingestion\n",
    "# Load shared logging utilities\n",
    "\n",
    "source_path = \"/FileStore/project/source/\"\n",
    "bronze_path = \"/FileStore/project/bronze/\"\n",
    "notebook_name = \"bronze_ingestion\"\n",
    "\n",
    "log_event(\"bronze\", notebook_name, \"START\", 0, \"RUNNING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7324efbe-d292-4639-b518-73d32cd5ed04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.2: Read raw sales data\n",
    "\n",
    "sales_raw = spark.read.option(\"header\", True).csv(\n",
    "    source_path + \"sales_transactions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a366ea18-6490-468a-9435-2f5bf18584c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.3: Add ingestion metadata\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "sales_bronze = (\n",
    "    sales_raw\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_system\", lit(\"manual_csv\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f704dc1-79de-49aa-a058-00e0b2d90341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.4: Write Bronze Delta table with error handling\n",
    "\n",
    "try:\n",
    "    sales_bronze.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(bronze_path + \"sales\")\n",
    "\n",
    "    count = sales_bronze.count()\n",
    "\n",
    "    log_event(\"bronze\", notebook_name, \"END\", count, \"SUCCESS\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_event(\"bronze\", notebook_name, \"ERROR\", 0, \"FAILED\", str(e))\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}