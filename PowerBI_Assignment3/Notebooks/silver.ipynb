{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7809d741-8dcb-453d-91e8-8ed1b23551c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Logging setup (self-contained)\n",
    "# -------------------------------\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import uuid\n",
    "\n",
    "LOG_PATH = \"/FileStore/project/logs/pipeline_logs\"\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField(\"run_id\", StringType()),\n",
    "    StructField(\"pipeline_layer\", StringType()),\n",
    "    StructField(\"notebook_name\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),   # START / END / ERROR / REJECTED\n",
    "    StructField(\"record_count\", LongType()),\n",
    "    StructField(\"status\", StringType()),       # RUNNING / SUCCESS / FAILED\n",
    "    StructField(\"error_message\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Create logging table if it does not exist\n",
    "if not spark._jsparkSession.catalog().tableExists(\"delta.`/FileStore/project/logs/pipeline_logs`\"):\n",
    "    spark.createDataFrame([], log_schema) \\\n",
    "        .write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(LOG_PATH)\n",
    "\n",
    "def log_event(layer, notebook, event_type, record_count, status, error_msg=None):\n",
    "\n",
    "    row = [(\n",
    "        str(uuid.uuid4()),\n",
    "        layer,\n",
    "        notebook,\n",
    "        event_type,\n",
    "        int(record_count),\n",
    "        status,\n",
    "        error_msg,          # can be None safely now\n",
    "        None                # placeholder for timestamp\n",
    "    )]\n",
    "\n",
    "    df = spark.createDataFrame(row, schema=log_schema) \\\n",
    "              .withColumn(\"event_timestamp\", current_timestamp())\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").save(LOG_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ef1760-456d-447f-a154-47f856b980a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.1: Start Silver processing\n",
    "\n",
    "silver_path = \"/FileStore/project/silver/\"\n",
    "quarantine_path = \"/FileStore/project/quarantine/\"\n",
    "notebook_name = \"silver_processing\"\n",
    "\n",
    "log_event(\"silver\", notebook_name, \"START\", 0, \"RUNNING\")\n",
    "\n",
    "bronze_sales = spark.read.format(\"delta\").load(\n",
    "    \"/FileStore/project/bronze/sales\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9fdbb9-26e3-4dab-aa40-285fd172eea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.2: Remove duplicate transactions\n",
    "\n",
    "silver_dedup = bronze_sales.dropDuplicates([\"transaction_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd3ecd9-0587-4cc4-9ff9-5166b212c9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.3: Recalculate totals correctly\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_calc = silver_dedup.withColumn(\n",
    "    \"correct_total\",\n",
    "    col(\"quantity\") * col(\"unit_price\") - col(\"discount\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517b9fe6-46e9-4c43-89e1-1b0bdce59498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.4: Apply validation rules\n",
    "\n",
    "valid_sales = silver_calc.filter(\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unit_price\") > 0) &\n",
    "    (col(\"store_id\").isin(\"S01\",\"S02\",\"S03\"))\n",
    ")\n",
    "\n",
    "invalid_sales = silver_calc.subtract(valid_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e24653-d2e6-4931-be95-dcd96fb9d8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4.5: Write outputs and log metrics\n",
    "\n",
    "try:\n",
    "    valid_sales.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(silver_path + \"sales\")\n",
    "\n",
    "    invalid_sales.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(quarantine_path + \"sales\")\n",
    "\n",
    "    log_event(\"silver\", notebook_name, \"END\",\n",
    "              valid_sales.count(), \"SUCCESS\")\n",
    "\n",
    "    log_event(\"silver\", notebook_name, \"REJECTED\",\n",
    "              invalid_sales.count(), \"SUCCESS\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_event(\"silver\", notebook_name, \"ERROR\", 0, \"FAILED\", str(e))\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}