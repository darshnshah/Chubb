{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f064b0e6-88c4-46cb-9a7f-98516c0bcc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Logging setup (self-contained)\n",
    "# -------------------------------\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import uuid\n",
    "\n",
    "LOG_PATH = \"/FileStore/project/logs/pipeline_logs\"\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField(\"run_id\", StringType()),\n",
    "    StructField(\"pipeline_layer\", StringType()),\n",
    "    StructField(\"notebook_name\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),   # START / END / ERROR / REJECTED\n",
    "    StructField(\"record_count\", LongType()),\n",
    "    StructField(\"status\", StringType()),       # RUNNING / SUCCESS / FAILED\n",
    "    StructField(\"error_message\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Create logging table if it does not exist\n",
    "if not spark._jsparkSession.catalog().tableExists(\"delta.`/FileStore/project/logs/pipeline_logs`\"):\n",
    "    spark.createDataFrame([], log_schema) \\\n",
    "        .write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(LOG_PATH)\n",
    "\n",
    "def log_event(layer, notebook, event_type, record_count, status, error_msg=None):\n",
    "\n",
    "    row = [(\n",
    "        str(uuid.uuid4()),\n",
    "        layer,\n",
    "        notebook,\n",
    "        event_type,\n",
    "        int(record_count),\n",
    "        status,\n",
    "        error_msg,          # can be None safely now\n",
    "        None                # placeholder for timestamp\n",
    "    )]\n",
    "\n",
    "    df = spark.createDataFrame(row, schema=log_schema) \\\n",
    "              .withColumn(\"event_timestamp\", current_timestamp())\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").save(LOG_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b53828-77b2-4309-ad2f-2d51e9f8ab4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6.1: Initialize Gold layer processing\n",
    "\n",
    "notebook_name = \"gold_aggregation\"\n",
    "log_event(\"gold\", notebook_name, \"START\", 0, \"RUNNING\")\n",
    "\n",
    "# Source (Silver)\n",
    "silver_sales_path = \"/FileStore/project/silver/sales\"\n",
    "\n",
    "sales = spark.read.format(\"delta\").load(silver_sales_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67ec2c3-e0ce-477b-b7bf-7275650ebc18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 6.2: Ensure Gold schema exists in hive_metastore\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS hive_metastore.project_gold\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a8cd20-a096-4320-849a-09ab70f5bfbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6.3: Create Daily Sales Summary\n",
    "\n",
    "gold_daily_sales = (\n",
    "    sales\n",
    "    .groupBy(\"transaction_date\")\n",
    "    .sum(\"correct_total\")\n",
    "    .withColumnRenamed(\"sum(correct_total)\", \"daily_revenue\")\n",
    ")\n",
    "\n",
    "gold_daily_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"hive_metastore.project_gold.daily_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530034b0-3bb9-4482-b1dd-2be470288593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6.4: Create Monthly Revenue by Region\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "stores = spark.read.option(\"header\", True).csv(\n",
    "    \"/FileStore/project/source/stores.csv\"\n",
    ")\n",
    "\n",
    "gold_monthly_region = (\n",
    "    sales.join(stores, \"store_id\")\n",
    "    .withColumn(\"month\", month(\"transaction_date\"))\n",
    "    .groupBy(\"month\", \"region\")\n",
    "    .sum(\"correct_total\")\n",
    "    .withColumnRenamed(\"sum(correct_total)\", \"monthly_revenue\")\n",
    ")\n",
    "\n",
    "gold_monthly_region.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"hive_metastore.project_gold.monthly_region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd17ae2-2fcf-4334-9e55-67c895e49308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6.5: Create Product Performance Metrics\n",
    "\n",
    "products = spark.read.option(\"header\", True).csv(\n",
    "    \"/FileStore/project/source/products.csv\"\n",
    ")\n",
    "\n",
    "gold_product_performance = (\n",
    "    sales.join(products, \"product_id\")\n",
    "    .groupBy(\"product_name\", \"category\")\n",
    "    .sum(\"correct_total\")\n",
    "    .withColumnRenamed(\"sum(correct_total)\", \"total_revenue\")\n",
    ")\n",
    "\n",
    "gold_product_performance.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"hive_metastore.project_gold.product_performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bf8b83-c8af-42fe-8604-f9fd56d27c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6.6: Log successful Gold completion\n",
    "\n",
    "log_event(\n",
    "    layer=\"gold\",\n",
    "    notebook=notebook_name,\n",
    "    event_type=\"END\",\n",
    "    record_count=gold_daily_sales.count(),\n",
    "    status=\"SUCCESS\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}